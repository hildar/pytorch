{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you \n# create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-23T07:29:57.172218Z","iopub.execute_input":"2022-08-23T07:29:57.172706Z","iopub.status.idle":"2022-08-23T07:29:57.186454Z","shell.execute_reply.started":"2022-08-23T07:29:57.172666Z","shell.execute_reply":"2022-08-23T07:29:57.185335Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:29:58.074102Z","iopub.execute_input":"2022-08-23T07:29:58.074469Z","iopub.status.idle":"2022-08-23T07:29:58.079704Z","shell.execute_reply.started":"2022-08-23T07:29:58.074440Z","shell.execute_reply":"2022-08-23T07:29:58.078441Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/twitter-sentiment-analysis-hatred-speech/'\ndf = pd.read_csv(PATH + \"train.csv\", index_col=0)\n# df_test = pd.read_csv(PATH + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:31:44.842585Z","iopub.execute_input":"2022-08-23T07:31:44.843300Z","iopub.status.idle":"2022-08-23T07:31:44.913600Z","shell.execute_reply.started":"2022-08-23T07:31:44.843261Z","shell.execute_reply":"2022-08-23T07:31:44.912359Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df, random_state=42, test_size=0.3)\ndf_train.shape, df_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:32:45.420841Z","iopub.execute_input":"2022-08-23T07:32:45.421245Z","iopub.status.idle":"2022-08-23T07:32:45.435247Z","shell.execute_reply.started":"2022-08-23T07:32:45.421214Z","shell.execute_reply":"2022-08-23T07:32:45.434147Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:32:46.573810Z","iopub.execute_input":"2022-08-23T07:32:46.574533Z","iopub.status.idle":"2022-08-23T07:32:46.584346Z","shell.execute_reply.started":"2022-08-23T07:32:46.574486Z","shell.execute_reply":"2022-08-23T07:32:46.583519Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_val.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:32:49.040998Z","iopub.execute_input":"2022-08-23T07:32:49.041405Z","iopub.status.idle":"2022-08-23T07:32:49.049684Z","shell.execute_reply.started":"2022-08-23T07:32:49.041374Z","shell.execute_reply":"2022-08-23T07:32:49.048496Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"max_words = 2000\nmax_len = 20\nnum_classes = 1\n\n# Training\nepochs = 5\nbatch_size = 512\nprint_batch_n = 100","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:35:41.866904Z","iopub.execute_input":"2022-08-23T07:35:41.867368Z","iopub.status.idle":"2022-08-23T07:35:41.873094Z","shell.execute_reply.started":"2022-08-23T07:35:41.867335Z","shell.execute_reply":"2022-08-23T07:35:41.872198Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Предобработка","metadata":{}},{"cell_type":"code","source":"!pip install -q stop-words pymorphy2","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:36:54.381213Z","iopub.execute_input":"2022-08-23T07:36:54.381646Z","iopub.status.idle":"2022-08-23T07:37:05.198000Z","shell.execute_reply.started":"2022-08-23T07:36:54.381609Z","shell.execute_reply":"2022-08-23T07:37:05.196807Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\nfrom stop_words import get_stop_words\nfrom pymorphy2 import MorphAnalyzer\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:37:14.351608Z","iopub.execute_input":"2022-08-23T07:37:14.352055Z","iopub.status.idle":"2022-08-23T07:37:14.381430Z","shell.execute_reply.started":"2022-08-23T07:37:14.352015Z","shell.execute_reply":"2022-08-23T07:37:14.380221Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sw = set(get_stop_words(\"en\"))\nsw.add('user')  \npuncts = set(punctuation)\nmorpher = MorphAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:45:02.627619Z","iopub.execute_input":"2022-08-23T08:45:02.628014Z","iopub.status.idle":"2022-08-23T08:45:02.894000Z","shell.execute_reply.started":"2022-08-23T08:45:02.627981Z","shell.execute_reply":"2022-08-23T08:45:02.892712Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# sw","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:45:21.695683Z","iopub.execute_input":"2022-08-23T08:45:21.696536Z","iopub.status.idle":"2022-08-23T08:45:21.701130Z","shell.execute_reply.started":"2022-08-23T08:45:21.696492Z","shell.execute_reply":"2022-08-23T08:45:21.699902Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(txt):\n    txt = str(txt)\n    txt = \"\".join(c for c in txt if c not in puncts)\n    txt = txt.lower()\n    txt = re.sub(\"not\\s\", \"not\", txt)\n#     txt = re.sub(\"(.\\\\x[0-9]*.|\\Z)(\\\\x[0-9]*.|\\Z|)\", \" \", txt)\n#     txt = \"\".join(c for c in txt if c not in puncts)\n    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n    \n    return \" \".join(txt)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:54:20.313522Z","iopub.execute_input":"2022-08-23T08:54:20.313971Z","iopub.status.idle":"2022-08-23T08:54:20.321662Z","shell.execute_reply.started":"2022-08-23T08:54:20.313936Z","shell.execute_reply":"2022-08-23T08:54:20.320824Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"a = df_train.tweet.iloc[0]\na","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:54:21.206988Z","iopub.execute_input":"2022-08-23T08:54:21.207407Z","iopub.status.idle":"2022-08-23T08:54:21.215389Z","shell.execute_reply.started":"2022-08-23T08:54:21.207375Z","shell.execute_reply":"2022-08-23T08:54:21.214190Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"df_train['tweet'].iloc[:1].apply(preprocess_text).values","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:54:23.224435Z","iopub.execute_input":"2022-08-23T08:54:23.225238Z","iopub.status.idle":"2022-08-23T08:54:23.234127Z","shell.execute_reply.started":"2022-08-23T08:54:23.225199Z","shell.execute_reply":"2022-08-23T08:54:23.232956Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \ntqdm.pandas()\n\ndf_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\ndf_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:55:34.633595Z","iopub.execute_input":"2022-08-23T08:55:34.634698Z","iopub.status.idle":"2022-08-23T08:55:42.649271Z","shell.execute_reply.started":"2022-08-23T08:55:34.634647Z","shell.execute_reply":"2022-08-23T08:55:42.648085Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"train_corpus = \" \".join(df_train[\"tweet\"])\ntrain_corpus = train_corpus.lower()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:55:58.054449Z","iopub.execute_input":"2022-08-23T08:55:58.055689Z","iopub.status.idle":"2022-08-23T08:55:58.076038Z","shell.execute_reply.started":"2022-08-23T08:55:58.055638Z","shell.execute_reply":"2022-08-23T08:55:58.074674Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"punkt\")\n\ntokens = word_tokenize(train_corpus)\ntokens[:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:56:19.827267Z","iopub.execute_input":"2022-08-23T08:56:19.827720Z","iopub.status.idle":"2022-08-23T08:56:21.495988Z","shell.execute_reply.started":"2022-08-23T08:56:19.827683Z","shell.execute_reply":"2022-08-23T08:56:21.494794Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"Отфильтруем данные\n\nи соберём в корпус N наиболее частых токенов","metadata":{}},{"cell_type":"code","source":"tokens_filtered = [word for word in tokens if word.isalnum()]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:57:50.443336Z","iopub.execute_input":"2022-08-23T08:57:50.443786Z","iopub.status.idle":"2022-08-23T08:57:50.476427Z","shell.execute_reply.started":"2022-08-23T08:57:50.443750Z","shell.execute_reply":"2022-08-23T08:57:50.475432Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\n\ndist = FreqDist(tokens_filtered)\ntokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\nlen(tokens_filtered_top)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:58:19.256099Z","iopub.execute_input":"2022-08-23T08:58:19.257118Z","iopub.status.idle":"2022-08-23T08:58:19.416210Z","shell.execute_reply.started":"2022-08-23T08:58:19.257075Z","shell.execute_reply":"2022-08-23T08:58:19.415320Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"tokens_filtered_top[:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:58:27.920632Z","iopub.execute_input":"2022-08-23T08:58:27.921079Z","iopub.status.idle":"2022-08-23T08:58:27.928218Z","shell.execute_reply.started":"2022-08-23T08:58:27.921042Z","shell.execute_reply":"2022-08-23T08:58:27.927287Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\nvocabulary","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:59:38.919738Z","iopub.execute_input":"2022-08-23T08:59:38.920490Z","iopub.status.idle":"2022-08-23T08:59:38.958335Z","shell.execute_reply.started":"2022-08-23T08:59:38.920440Z","shell.execute_reply":"2022-08-23T08:59:38.957253Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef text_to_sequence(text, maxlen):\n    result = []\n    tokens = word_tokenize(text.lower())\n    tokens_filtered = [word for word in tokens if word.isalnum()]\n    for word in tokens_filtered:\n        if word in vocabulary:\n            result.append(vocabulary[word])\n\n    padding = [0] * (maxlen-len(result))\n    return result[-maxlen:] + padding","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:59:52.145684Z","iopub.execute_input":"2022-08-23T08:59:52.146100Z","iopub.status.idle":"2022-08-23T08:59:52.152818Z","shell.execute_reply.started":"2022-08-23T08:59:52.146064Z","shell.execute_reply":"2022-08-23T08:59:52.152029Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"%%time\nx_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]])\nx_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"tweet\"]])","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:00:45.102744Z","iopub.execute_input":"2022-08-23T09:00:45.103179Z","iopub.status.idle":"2022-08-23T09:00:48.939713Z","shell.execute_reply.started":"2022-08-23T09:00:45.103145Z","shell.execute_reply":"2022-08-23T09:00:48.938900Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:01:17.558406Z","iopub.execute_input":"2022-08-23T09:01:17.558810Z","iopub.status.idle":"2022-08-23T09:01:17.566006Z","shell.execute_reply.started":"2022-08-23T09:01:17.558776Z","shell.execute_reply":"2022-08-23T09:01:17.564750Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"df_train.tweet.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:02:20.776702Z","iopub.execute_input":"2022-08-23T09:02:20.777343Z","iopub.status.idle":"2022-08-23T09:02:20.784281Z","shell.execute_reply.started":"2022-08-23T09:02:20.777307Z","shell.execute_reply":"2022-08-23T09:02:20.783034Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"x_train[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:01:42.593776Z","iopub.execute_input":"2022-08-23T09:01:42.594550Z","iopub.status.idle":"2022-08-23T09:01:42.600243Z","shell.execute_reply.started":"2022-08-23T09:01:42.594513Z","shell.execute_reply":"2022-08-23T09:01:42.599371Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self, vocab_size=2000, embedding_dim=128, out_channel=128, num_classes=1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=2)\n        self.conv_2 = nn.Conv1d(embedding_dim, out_channel, kernel_size=3)\n        self.pool = nn.MaxPool1d(2)\n        self.relu = nn.ReLU()\n        self.linear_1 = nn.Linear(out_channel, out_channel // 2)\n        self.linear_2 = nn.Linear(out_channel // 2, num_classes)\n        \n    def forward(self, x):        \n        output = self.embedding(x) # B, L, E\n        #                       B  E  L         \n        output = output.permute(0, 2, 1)\n        output = self.conv_1(output)\n        output = self.relu(output)\n        output = self.pool(output)\n\n        output = self.conv_2(output)\n        output = self.relu(output)\n        output = self.pool(output)\n        output = torch.max(output, axis=2).values\n        output = self.linear_1(output)\n        output = self.relu(output)\n        output = self.linear_2(output)\n        output = F.sigmoid(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:03:21.995568Z","iopub.execute_input":"2022-08-23T09:03:21.996425Z","iopub.status.idle":"2022-08-23T09:03:23.611575Z","shell.execute_reply.started":"2022-08-23T09:03:21.996387Z","shell.execute_reply":"2022-08-23T09:03:23.610369Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n\nclass DataWrapper(Dataset):\n    def __init__(self, data, target, transform=None):\n        self.data = torch.from_numpy(data).long()\n        self.target = torch.from_numpy(target).long()\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        \n        if self.transform:\n            x = self.transform(x)\n            \n        return x, y\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:03:42.235869Z","iopub.execute_input":"2022-08-23T09:03:42.236693Z","iopub.status.idle":"2022-08-23T09:03:42.243709Z","shell.execute_reply.started":"2022-08-23T09:03:42.236655Z","shell.execute_reply":"2022-08-23T09:03:42.242641Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataWrapper(x_train, df_train['label'].values)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = DataWrapper(x_val, df_val['label'].values)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:04:44.573973Z","iopub.execute_input":"2022-08-23T09:04:44.574455Z","iopub.status.idle":"2022-08-23T09:04:44.582166Z","shell.execute_reply.started":"2022-08-23T09:04:44.574406Z","shell.execute_reply":"2022-08-23T09:04:44.580923Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"for x, l in train_loader:\n    print(x.shape)\n    print(l.shape)\n    print(l[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:04:57.715998Z","iopub.execute_input":"2022-08-23T09:04:57.716379Z","iopub.status.idle":"2022-08-23T09:04:57.735116Z","shell.execute_reply.started":"2022-08-23T09:04:57.716350Z","shell.execute_reply":"2022-08-23T09:04:57.734262Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"model = Net(vocab_size=max_words)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:05:07.955777Z","iopub.execute_input":"2022-08-23T09:05:07.956635Z","iopub.status.idle":"2022-08-23T09:05:07.971364Z","shell.execute_reply.started":"2022-08-23T09:05:07.956598Z","shell.execute_reply":"2022-08-23T09:05:07.970187Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"print(model)\nprint(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:05:18.866366Z","iopub.execute_input":"2022-08-23T09:05:18.866801Z","iopub.status.idle":"2022-08-23T09:05:18.872158Z","shell.execute_reply.started":"2022-08-23T09:05:18.866766Z","shell.execute_reply":"2022-08-23T09:05:18.871183Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:05:26.977647Z","iopub.execute_input":"2022-08-23T09:05:26.978408Z","iopub.status.idle":"2022-08-23T09:05:26.984536Z","shell.execute_reply.started":"2022-08-23T09:05:26.978370Z","shell.execute_reply":"2022-08-23T09:05:26.983442Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nmodel.train()\nth = 0.5\n\ntrain_loss_history = []\ntest_loss_history = []\n\n\nfor epoch in range(epochs):  \n    running_items, running_right = 0.0, 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        \n        # обнуляем градиент\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        loss = criterion(outputs, labels.float().view(-1, 1))\n        loss.backward()\n        optimizer.step()\n\n        # подсчет ошибки на обучении\n        loss = loss.item()\n        running_items += len(labels)\n        # подсчет метрики на обучении\n        pred_labels = torch.squeeze((outputs > th).int())\n        running_right += (labels == pred_labels).sum()\n        \n    # выводим статистику о процессе обучения\n    model.eval()\n    \n    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n            f'Step [{i + 1}/{len(train_loader)}]. ' \\\n            f'Loss: {loss:.3f}. ' \\\n            f'Acc: {running_right / running_items:.3f}', end='. ')\n    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n    train_loss_history.append(loss)\n\n        # выводим статистику на тестовых данных\n    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n    for j, data in enumerate(val_loader):\n        test_labels = data[1].to(device)\n        test_outputs = model(data[0].to(device))\n        \n        # подсчет ошибки на тесте\n        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n        # подсчет метрики на тесте\n        test_running_total += len(data[1])\n        pred_test_labels = torch.squeeze((test_outputs > th).int())\n        test_running_right += (test_labels == pred_test_labels).sum()\n    \n    test_loss_history.append(test_loss.item())\n    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n    \n    model.train()\n        \nprint('Training is finished!')","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:07:58.566498Z","iopub.execute_input":"2022-08-23T09:07:58.566933Z","iopub.status.idle":"2022-08-23T09:08:21.776111Z","shell.execute_reply.started":"2022-08-23T09:07:58.566901Z","shell.execute_reply":"2022-08-23T09:08:21.775127Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.title('Loss history')\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.plot(train_loss_history, label='train')\nplt.plot(test_loss_history, label='test')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-08-23T09:08:25.586817Z","iopub.execute_input":"2022-08-23T09:08:25.587635Z","iopub.status.idle":"2022-08-23T09:08:25.859283Z","shell.execute_reply.started":"2022-08-23T09:08:25.587588Z","shell.execute_reply":"2022-08-23T09:08:25.858068Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}