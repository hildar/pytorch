{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you \n# create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-25T18:55:34.331589Z","iopub.execute_input":"2022-08-25T18:55:34.332059Z","iopub.status.idle":"2022-08-25T18:55:34.388766Z","shell.execute_reply.started":"2022-08-25T18:55:34.331977Z","shell.execute_reply":"2022-08-25T18:55:34.387271Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:34.391685Z","iopub.execute_input":"2022-08-25T18:55:34.393450Z","iopub.status.idle":"2022-08-25T18:55:35.015648Z","shell.execute_reply.started":"2022-08-25T18:55:34.393410Z","shell.execute_reply":"2022-08-25T18:55:35.014285Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/twitter-sentiment-analysis-hatred-speech/'\ndf = pd.read_csv(PATH + \"train.csv\", index_col=0)\n# df_test = pd.read_csv(PATH + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.018328Z","iopub.execute_input":"2022-08-25T18:55:35.019234Z","iopub.status.idle":"2022-08-25T18:55:35.186708Z","shell.execute_reply.started":"2022-08-25T18:55:35.019174Z","shell.execute_reply":"2022-08-25T18:55:35.185501Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df, random_state=42, test_size=0.3)\ndf_train.shape, df_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.191408Z","iopub.execute_input":"2022-08-25T18:55:35.191715Z","iopub.status.idle":"2022-08-25T18:55:35.214998Z","shell.execute_reply.started":"2022-08-25T18:55:35.191687Z","shell.execute_reply":"2022-08-25T18:55:35.213439Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.216678Z","iopub.execute_input":"2022-08-25T18:55:35.217235Z","iopub.status.idle":"2022-08-25T18:55:35.229914Z","shell.execute_reply.started":"2022-08-25T18:55:35.217177Z","shell.execute_reply":"2022-08-25T18:55:35.228141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_val.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.232325Z","iopub.execute_input":"2022-08-25T18:55:35.233344Z","iopub.status.idle":"2022-08-25T18:55:35.247954Z","shell.execute_reply.started":"2022-08-25T18:55:35.233306Z","shell.execute_reply":"2022-08-25T18:55:35.246249Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_words = 4000\nmax_len = 10\nnum_classes = 1\n\n# Training\nepochs = 10\nbatch_size = 512\nprint_batch_n = 100","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.250309Z","iopub.execute_input":"2022-08-25T18:55:35.250833Z","iopub.status.idle":"2022-08-25T18:55:35.260671Z","shell.execute_reply.started":"2022-08-25T18:55:35.250780Z","shell.execute_reply":"2022-08-25T18:55:35.258846Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Предобработка","metadata":{}},{"cell_type":"code","source":"!pip install -q stop-words pymorphy2","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:35.263196Z","iopub.execute_input":"2022-08-25T18:55:35.265039Z","iopub.status.idle":"2022-08-25T18:55:51.261273Z","shell.execute_reply.started":"2022-08-25T18:55:35.264984Z","shell.execute_reply":"2022-08-25T18:55:51.259812Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import re\nfrom string import punctuation\nfrom stop_words import get_stop_words\nfrom pymorphy2 import MorphAnalyzer\n\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:51.267267Z","iopub.execute_input":"2022-08-25T18:55:51.268477Z","iopub.status.idle":"2022-08-25T18:55:52.455793Z","shell.execute_reply.started":"2022-08-25T18:55:51.268430Z","shell.execute_reply":"2022-08-25T18:55:52.454223Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sw = set(get_stop_words(\"en\"))\nsw.add('user')  \npuncts = set(punctuation)\nmorpher = MorphAnalyzer()\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:52.458793Z","iopub.execute_input":"2022-08-25T18:55:52.459680Z","iopub.status.idle":"2022-08-25T18:55:52.946962Z","shell.execute_reply.started":"2022-08-25T18:55:52.459631Z","shell.execute_reply":"2022-08-25T18:55:52.945740Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# sw\nmorpher.parse('бегали')[0].normal_form","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:52.952900Z","iopub.execute_input":"2022-08-25T18:55:52.956130Z","iopub.status.idle":"2022-08-25T18:55:52.968772Z","shell.execute_reply.started":"2022-08-25T18:55:52.956074Z","shell.execute_reply":"2022-08-25T18:55:52.967410Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lemmatizer.lemmatize(\"rocks\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:52.974523Z","iopub.execute_input":"2022-08-25T18:55:52.978038Z","iopub.status.idle":"2022-08-25T18:55:54.849699Z","shell.execute_reply.started":"2022-08-25T18:55:52.977993Z","shell.execute_reply":"2022-08-25T18:55:54.848251Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(txt):\n    txt = str(txt)\n    txt = \"\".join(c for c in txt if c not in puncts)\n    txt = txt.lower()\n    txt = re.sub(\"not\\s\", \"not\", txt)\n#     txt = re.sub(\"(.\\\\x[0-9]*.|\\Z)(\\\\x[0-9]*.|\\Z|)\", \" \", txt)\n#     txt = \"\".join(c for c in txt if c not in puncts)\n#     txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n    txt = [lemmatizer.lemmatize(word) for word in txt.split() if word not in sw]\n    return \" \".join(txt)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:54.851912Z","iopub.execute_input":"2022-08-25T18:55:54.852374Z","iopub.status.idle":"2022-08-25T18:55:54.860957Z","shell.execute_reply.started":"2022-08-25T18:55:54.852316Z","shell.execute_reply":"2022-08-25T18:55:54.859254Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"a = df_train.tweet.iloc[0]\na","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:54.863684Z","iopub.execute_input":"2022-08-25T18:55:54.864246Z","iopub.status.idle":"2022-08-25T18:55:54.883752Z","shell.execute_reply.started":"2022-08-25T18:55:54.864163Z","shell.execute_reply":"2022-08-25T18:55:54.882352Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_train['tweet'].iloc[:1].apply(preprocess_text).values","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:54.885982Z","iopub.execute_input":"2022-08-25T18:55:54.887148Z","iopub.status.idle":"2022-08-25T18:55:54.901317Z","shell.execute_reply.started":"2022-08-25T18:55:54.887118Z","shell.execute_reply":"2022-08-25T18:55:54.899918Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \ntqdm.pandas()\n\ndf_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\ndf_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:54.904535Z","iopub.execute_input":"2022-08-25T18:55:54.906450Z","iopub.status.idle":"2022-08-25T18:55:56.819486Z","shell.execute_reply.started":"2022-08-25T18:55:54.906421Z","shell.execute_reply":"2022-08-25T18:55:56.817727Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_corpus = \" \".join(df_train[\"tweet\"])\ntrain_corpus = train_corpus.lower()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:56.821159Z","iopub.execute_input":"2022-08-25T18:55:56.822311Z","iopub.status.idle":"2022-08-25T18:55:56.844039Z","shell.execute_reply.started":"2022-08-25T18:55:56.822252Z","shell.execute_reply":"2022-08-25T18:55:56.842697Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"punkt\")\n\ntokens = word_tokenize(train_corpus)\ntokens[:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:56.847364Z","iopub.execute_input":"2022-08-25T18:55:56.848232Z","iopub.status.idle":"2022-08-25T18:55:57.663026Z","shell.execute_reply.started":"2022-08-25T18:55:56.848157Z","shell.execute_reply":"2022-08-25T18:55:57.661688Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Отфильтруем данные\n\nи соберём в корпус N наиболее частых токенов","metadata":{}},{"cell_type":"code","source":"tokens_filtered = [word for word in tokens if word.isalnum()]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.666073Z","iopub.execute_input":"2022-08-25T18:55:57.666586Z","iopub.status.idle":"2022-08-25T18:55:57.695732Z","shell.execute_reply.started":"2022-08-25T18:55:57.666539Z","shell.execute_reply":"2022-08-25T18:55:57.694248Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\n\ndist = FreqDist(tokens_filtered)\ntokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\nlen(tokens_filtered_top)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.697519Z","iopub.execute_input":"2022-08-25T18:55:57.699164Z","iopub.status.idle":"2022-08-25T18:55:57.844399Z","shell.execute_reply.started":"2022-08-25T18:55:57.699124Z","shell.execute_reply":"2022-08-25T18:55:57.843147Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokens_filtered_top[:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.846267Z","iopub.execute_input":"2022-08-25T18:55:57.846677Z","iopub.status.idle":"2022-08-25T18:55:57.856232Z","shell.execute_reply.started":"2022-08-25T18:55:57.846635Z","shell.execute_reply":"2022-08-25T18:55:57.854239Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n# vocabulary","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.858530Z","iopub.execute_input":"2022-08-25T18:55:57.859526Z","iopub.status.idle":"2022-08-25T18:55:57.868154Z","shell.execute_reply.started":"2022-08-25T18:55:57.859472Z","shell.execute_reply":"2022-08-25T18:55:57.866339Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef text_to_sequence(text, maxlen):\n    result = []\n    tokens = word_tokenize(text.lower())\n    tokens_filtered = [word for word in tokens if word.isalnum()]\n    for word in tokens_filtered:\n        if word in vocabulary:\n            result.append(vocabulary[word])\n\n    padding = [0] * (maxlen-len(result))\n    return result[-maxlen:] + padding","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.870121Z","iopub.execute_input":"2022-08-25T18:55:57.870782Z","iopub.status.idle":"2022-08-25T18:55:57.881808Z","shell.execute_reply.started":"2022-08-25T18:55:57.870742Z","shell.execute_reply":"2022-08-25T18:55:57.879823Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%%time\nx_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]])\nx_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"tweet\"]])","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:55:57.890625Z","iopub.execute_input":"2022-08-25T18:55:57.891849Z","iopub.status.idle":"2022-08-25T18:56:01.882931Z","shell.execute_reply.started":"2022-08-25T18:55:57.891807Z","shell.execute_reply":"2022-08-25T18:56:01.881376Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:01.884775Z","iopub.execute_input":"2022-08-25T18:56:01.885478Z","iopub.status.idle":"2022-08-25T18:56:01.895229Z","shell.execute_reply.started":"2022-08-25T18:56:01.885406Z","shell.execute_reply":"2022-08-25T18:56:01.893558Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_train.tweet.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:01.896939Z","iopub.execute_input":"2022-08-25T18:56:01.898694Z","iopub.status.idle":"2022-08-25T18:56:01.911369Z","shell.execute_reply.started":"2022-08-25T18:56:01.898661Z","shell.execute_reply":"2022-08-25T18:56:01.909840Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"x_train[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:01.913172Z","iopub.execute_input":"2022-08-25T18:56:01.913793Z","iopub.status.idle":"2022-08-25T18:56:01.923495Z","shell.execute_reply.started":"2022-08-25T18:56:01.913619Z","shell.execute_reply":"2022-08-25T18:56:01.921833Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:01.925722Z","iopub.execute_input":"2022-08-25T18:56:01.926166Z","iopub.status.idle":"2022-08-25T18:56:04.510676Z","shell.execute_reply.started":"2022-08-25T18:56:01.926119Z","shell.execute_reply":"2022-08-25T18:56:04.509267Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class LSTMFixedLen(nn.Module) :\n    def __init__(self, vocab_size=2000, embedding_dim=128, hidden_dim=128, use_last=True):\n        super().__init__()\n        self.use_last = use_last\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, ht = self.lstm(x)\n       \n        if self.use_last:\n            last_tensor = lstm_out[:,-1,:]\n        else:\n            # use mean\n            last_tensor = torch.mean(lstm_out[:,:], dim=1)\n    \n        out = self.linear(last_tensor)\n        # print(out.shape)\n        return torch.sigmoid(out)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.530142Z","iopub.execute_input":"2022-08-25T18:56:04.530809Z","iopub.status.idle":"2022-08-25T18:56:04.544634Z","shell.execute_reply.started":"2022-08-25T18:56:04.530769Z","shell.execute_reply":"2022-08-25T18:56:04.542902Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n\nclass DataWrapper(Dataset):\n    def __init__(self, data, target, transform=None):\n        self.data = torch.from_numpy(data).long()\n        self.target = torch.from_numpy(target).long()\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        \n        if self.transform:\n            x = self.transform(x)\n            \n        return x, y\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.546407Z","iopub.execute_input":"2022-08-25T18:56:04.546894Z","iopub.status.idle":"2022-08-25T18:56:04.560146Z","shell.execute_reply.started":"2022-08-25T18:56:04.546837Z","shell.execute_reply":"2022-08-25T18:56:04.558606Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataWrapper(x_train, df_train['label'].values)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = DataWrapper(x_val, df_val['label'].values)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.562042Z","iopub.execute_input":"2022-08-25T18:56:04.562491Z","iopub.status.idle":"2022-08-25T18:56:04.575028Z","shell.execute_reply.started":"2022-08-25T18:56:04.562448Z","shell.execute_reply":"2022-08-25T18:56:04.573750Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for x, l in train_loader:\n    print(x.shape)\n    print(l.shape)\n    print(l[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.577095Z","iopub.execute_input":"2022-08-25T18:56:04.578137Z","iopub.status.idle":"2022-08-25T18:56:04.603996Z","shell.execute_reply.started":"2022-08-25T18:56:04.578096Z","shell.execute_reply":"2022-08-25T18:56:04.602380Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# model = Net(vocab_size=max_words)\nmodel = LSTMFixedLen(vocab_size=max_words)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.605965Z","iopub.execute_input":"2022-08-25T18:56:04.606612Z","iopub.status.idle":"2022-08-25T18:56:04.711948Z","shell.execute_reply.started":"2022-08-25T18:56:04.606552Z","shell.execute_reply":"2022-08-25T18:56:04.710504Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(model)\nprint(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.715888Z","iopub.execute_input":"2022-08-25T18:56:04.716216Z","iopub.status.idle":"2022-08-25T18:56:04.726724Z","shell.execute_reply.started":"2022-08-25T18:56:04.716163Z","shell.execute_reply":"2022-08-25T18:56:04.725249Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.728701Z","iopub.execute_input":"2022-08-25T18:56:04.729305Z","iopub.status.idle":"2022-08-25T18:56:04.739876Z","shell.execute_reply.started":"2022-08-25T18:56:04.729268Z","shell.execute_reply":"2022-08-25T18:56:04.738584Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from torchmetrics import F1Score\n# f1 = F1Score(num_classes=2)\nf1 = F1Score().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:04.741675Z","iopub.execute_input":"2022-08-25T18:56:04.741977Z","iopub.status.idle":"2022-08-25T18:56:09.849074Z","shell.execute_reply.started":"2022-08-25T18:56:04.741951Z","shell.execute_reply":"2022-08-25T18:56:09.847672Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:09.851054Z","iopub.execute_input":"2022-08-25T18:56:09.851598Z","iopub.status.idle":"2022-08-25T18:56:09.864801Z","shell.execute_reply.started":"2022-08-25T18:56:09.851527Z","shell.execute_reply":"2022-08-25T18:56:09.863050Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nmodel.train()\nth = 0.5\n\ntrain_loss_history = []\ntest_loss_history = []\n\n\nfor epoch in range(epochs):  \n    running_items, running_right = 0.0, 0.0\n    f1_res = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        \n        # обнуляем градиент\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        loss = criterion(outputs, labels.float().view(-1, 1))\n        loss.backward()\n        optimizer.step()\n\n        # подсчет ошибки на обучении\n        loss = loss.item()\n        running_items += len(labels)\n        # подсчет метрики на обучении\n        pred_labels = torch.squeeze((outputs > th).int())\n        running_right += (labels == pred_labels).sum()\n        f1_res += f1(outputs, labels)\n        \n    # выводим статистику о процессе обучения\n    model.eval()\n    \n    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n            f'Step [{i + 1}/{len(train_loader)}]. ' \\\n            f'Loss: {loss:.3f}. ' \\\n            f'F1: {f1_res / len(train_loader):.4f}. ' \\\n            f'Acc: {running_right / running_items:.3f}', end='. ')\n    running_items, running_right = 0.0, 0.0\n    f1_res = 0.0\n    train_loss_history.append(loss)\n\n        # выводим статистику на тестовых данных\n    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n    test_f1_res = 0\n    for j, data in enumerate(val_loader):\n        test_labels = data[1].to(device)\n        test_outputs = model(data[0].to(device))\n        \n        # подсчет ошибки на тесте\n        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n        # подсчет метрики на тесте\n        test_running_total += len(data[1])\n        pred_test_labels = torch.squeeze((test_outputs > th).int())\n        test_running_right += (test_labels == pred_test_labels).sum()\n        test_f1_res += f1(test_outputs, test_labels)\n    \n    test_loss_history.append(test_loss.item())\n    print(f'Test loss: {test_loss:.3f}. ' \\\n          f'Test F1: {test_f1_res / len(val_loader):.4f}. ' \\\n          f'Test acc: {test_running_right / test_running_total:.3f}')\n    \n    model.train()\n        \nprint('Training is finished!')","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:56:09.867304Z","iopub.execute_input":"2022-08-25T18:56:09.867818Z","iopub.status.idle":"2022-08-25T18:57:04.903877Z","shell.execute_reply.started":"2022-08-25T18:56:09.867778Z","shell.execute_reply":"2022-08-25T18:57:04.902446Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.title('Loss history')\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.plot(train_loss_history, label='train')\nplt.plot(test_loss_history, label='test')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-08-25T18:57:04.907635Z","iopub.execute_input":"2022-08-25T18:57:04.907977Z","iopub.status.idle":"2022-08-25T18:57:05.194868Z","shell.execute_reply.started":"2022-08-25T18:57:04.907948Z","shell.execute_reply":"2022-08-25T18:57:05.193259Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}