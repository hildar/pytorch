{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when \n# you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-20T21:16:00.631522Z","iopub.execute_input":"2022-08-20T21:16:00.631941Z","iopub.status.idle":"2022-08-20T21:16:00.664455Z","shell.execute_reply.started":"2022-08-20T21:16:00.631860Z","shell.execute_reply":"2022-08-20T21:16:00.663317Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:00.666018Z","iopub.execute_input":"2022-08-20T21:16:00.667902Z","iopub.status.idle":"2022-08-20T21:16:00.673050Z","shell.execute_reply.started":"2022-08-20T21:16:00.667864Z","shell.execute_reply":"2022-08-20T21:16:00.671882Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# HW Lesson 5\n\n1. Необходимо подготовить датасет https://www.kaggle.com/olekslu/makeup-lips-segmentation-28k-samples для обучения модели на сегментацию губ\n\n2. Обучить модель на выбор из [segmentation_models_pytorch](https://segmentation modelspytorch.readthedocs.io/en/latest/index.html)","metadata":{}},{"cell_type":"code","source":"! pip install --quiet segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:00.674876Z","iopub.execute_input":"2022-08-20T21:16:00.675273Z","iopub.status.idle":"2022-08-20T21:16:15.649418Z","shell.execute_reply.started":"2022-08-20T21:16:00.675239Z","shell.execute_reply":"2022-08-20T21:16:15.648251Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:15.653237Z","iopub.execute_input":"2022-08-20T21:16:15.653551Z","iopub.status.idle":"2022-08-20T21:16:25.521827Z","shell.execute_reply.started":"2022-08-20T21:16:15.653520Z","shell.execute_reply":"2022-08-20T21:16:25.520622Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\nfrom torch import nn\nfrom torchsummary import summary\nimport torchvision\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch import Linknet\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\nimport segmentation_models_pytorch.utils as smp_utils\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:25.526452Z","iopub.execute_input":"2022-08-20T21:16:25.526779Z","iopub.status.idle":"2022-08-20T21:16:30.383741Z","shell.execute_reply.started":"2022-08-20T21:16:25.526747Z","shell.execute_reply":"2022-08-20T21:16:30.382513Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH = Path('/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/')\nIMG_PATH = ROOT_PATH / '720p'\nMSK_PATH = ROOT_PATH / 'mask'\nDF_PATH = ROOT_PATH / 'list.csv'\nBACKBONE = 'resnet50'","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:30.385823Z","iopub.execute_input":"2022-08-20T21:16:30.386473Z","iopub.status.idle":"2022-08-20T21:16:30.393665Z","shell.execute_reply.started":"2022-08-20T21:16:30.386428Z","shell.execute_reply":"2022-08-20T21:16:30.391640Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(DF_PATH)\nprint(f'Shape {df.shape}')\nprint(f'Images in folder - {len(os.listdir(MSK_PATH))}\\nMasks in folder - {len(os.listdir(IMG_PATH))}')","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:30.395104Z","iopub.execute_input":"2022-08-20T21:16:30.395538Z","iopub.status.idle":"2022-08-20T21:16:32.948063Z","shell.execute_reply.started":"2022-08-20T21:16:30.395502Z","shell.execute_reply":"2022-08-20T21:16:32.946987Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:32.951273Z","iopub.execute_input":"2022-08-20T21:16:32.952932Z","iopub.status.idle":"2022-08-20T21:16:32.976104Z","shell.execute_reply.started":"2022-08-20T21:16:32.952888Z","shell.execute_reply":"2022-08-20T21:16:32.975232Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_val_df, test_df = train_test_split(df, random_state=42)\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)\n\ntrain_val_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:32.977257Z","iopub.execute_input":"2022-08-20T21:16:32.977525Z","iopub.status.idle":"2022-08-20T21:16:33.004703Z","shell.execute_reply.started":"2022-08-20T21:16:32.977500Z","shell.execute_reply":"2022-08-20T21:16:33.003642Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Make dataset","metadata":{}},{"cell_type":"code","source":"class LipsDataset(torch.utils.data.DataLoader):\n    IMG = 'filename'\n    MSK = 'mask'\n    data_type = 'float32' #'uint8'\n    \n    def __init__(self, mask_path: Path, img_path: Path, \n                 df: pd.DataFrame, preprocessing=None):\n        self.mask_path = mask_path\n        self.img_path = img_path\n        self.df = df\n        self.preprocessing = preprocessing\n        \n        tqdm.pandas()\n        print('Check if is sended imgage exist...')\n        self.df[self.IMG] = self.df[self.IMG]\\\n            .progress_apply(lambda f: img_path / f if os.path.exists(img_path / f) else np.NaN)\n        \n        print('Check if is sended mask exist...')\n        self.df[self.MSK] = self.df[self.MSK]\\\n            .progress_apply(lambda f: mask_path / f if os.path.exists(mask_path / f) else np.NaN)\n        self.df.dropna(axis=0, inplace=True)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def  __getitem__(self, indx):\n        # read data from folders\n        # https://tproger.ru/translations/opencv-python-guide/\n        image = cv2.imread(str(self.df.iloc[indx][self.IMG]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (256, 256))\n        # type does not matter because of torchvision.transforms.ToTensor() set float64\n        image = image.astype(self.data_type)\n        \n        mask = cv2.imread(str(self.df.iloc[indx][self.MSK]))\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        mask = cv2.resize(mask, (256, 256))\n        mask = mask / 255\n        mask = mask.astype(self.data_type)\n        \n        # apply preprocessing\n#         if self.preprocessing:\n#             sample = self.preprocessing(image=image, mask=mask)\n#             image, mask = sample['image'], sample['mask']\n            \n        if self.preprocessing:\n            image = self.preprocessing(image)\n#             image = torch.as_tensor(image)\n        else:\n            # normalyze to [0, 1]\n#             image = torch.as_tensor(image) / 255.0\n            image = image / 255.0\n            \n        return torchvision.transforms.ToTensor()(image.astype(self.data_type)),\\\n                torchvision.transforms.ToTensor()(mask.astype(self.data_type))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:33.007783Z","iopub.execute_input":"2022-08-20T21:16:33.008046Z","iopub.status.idle":"2022-08-20T21:16:33.020311Z","shell.execute_reply.started":"2022-08-20T21:16:33.008022Z","shell.execute_reply":"2022-08-20T21:16:33.019322Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def img_show(**images) -> None:\n    n = len(images)\n    plt.figure(figsize=(14, 5))\n    \n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n#         plt.imshow(image)\n        plt.imshow(image.permute(1, 2, 0))\n        plt.title(name)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:33.023106Z","iopub.execute_input":"2022-08-20T21:16:33.023932Z","iopub.status.idle":"2022-08-20T21:16:33.034359Z","shell.execute_reply.started":"2022-08-20T21:16:33.023902Z","shell.execute_reply":"2022-08-20T21:16:33.033452Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"transforms = smp.encoders.get_preprocessing_fn(BACKBONE, pretrained='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:33.039114Z","iopub.execute_input":"2022-08-20T21:16:33.040110Z","iopub.status.idle":"2022-08-20T21:16:33.045079Z","shell.execute_reply.started":"2022-08-20T21:16:33.040073Z","shell.execute_reply":"2022-08-20T21:16:33.043931Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset = LipsDataset(mask_path=MSK_PATH,\n                            img_path=IMG_PATH, \n                            df=train_df,\n                            preprocessing=transforms)\nval_dataset = LipsDataset(mask_path=MSK_PATH,\n                            img_path=IMG_PATH, \n                            df=val_df,\n                            preprocessing=transforms)\n\ntest_dataset = LipsDataset(mask_path=MSK_PATH,\n                            img_path=IMG_PATH, \n                            df=test_df,\n                            preprocessing=transforms)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:16:33.046743Z","iopub.execute_input":"2022-08-20T21:16:33.047099Z","iopub.status.idle":"2022-08-20T21:19:19.605877Z","shell.execute_reply.started":"2022-08-20T21:16:33.047065Z","shell.execute_reply":"2022-08-20T21:19:19.604482Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# check an image and mask\nk = torch.randint(0,train_dataset.__len__(),(1,1)).item()\nprint(f'Image {k} will be shown below.')\nimage, mask = train_dataset[k]\nimg_show(image=image, true_mask=mask)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:19.609112Z","iopub.execute_input":"2022-08-20T21:19:19.609513Z","iopub.status.idle":"2022-08-20T21:19:20.193671Z","shell.execute_reply.started":"2022-08-20T21:19:19.609473Z","shell.execute_reply":"2022-08-20T21:19:20.192532Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=8, \n                                           shuffle=True, \n                                           num_workers=2)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n                                         batch_size=4, \n                                         num_workers=2)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=4, \n                                          num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:20.195411Z","iopub.execute_input":"2022-08-20T21:19:20.195788Z","iopub.status.idle":"2022-08-20T21:19:20.202884Z","shell.execute_reply.started":"2022-08-20T21:19:20.195750Z","shell.execute_reply":"2022-08-20T21:19:20.201899Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:20.204537Z","iopub.execute_input":"2022-08-20T21:19:20.205364Z","iopub.status.idle":"2022-08-20T21:19:20.290165Z","shell.execute_reply.started":"2022-08-20T21:19:20.205324Z","shell.execute_reply":"2022-08-20T21:19:20.289129Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Linknet(encoder_name=BACKBONE, \n               encoder_weights='imagenet', \n               classes=1,\n               activation='sigmoid').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:20.291865Z","iopub.execute_input":"2022-08-20T21:19:20.292961Z","iopub.status.idle":"2022-08-20T21:19:31.918148Z","shell.execute_reply.started":"2022-08-20T21:19:20.292921Z","shell.execute_reply":"2022-08-20T21:19:31.917083Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"criterion = smp_utils.losses.DiceLoss()\nmetric = [smp_utils.metrics.IoU(),]\n\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.005)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:31.919458Z","iopub.execute_input":"2022-08-20T21:19:31.919842Z","iopub.status.idle":"2022-08-20T21:19:31.926564Z","shell.execute_reply.started":"2022-08-20T21:19:31.919803Z","shell.execute_reply":"2022-08-20T21:19:31.925637Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# path = '../input/lips-seg-weights/best_weights.pth'\n# model.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:31.927900Z","iopub.execute_input":"2022-08-20T21:19:31.929730Z","iopub.status.idle":"2022-08-20T21:19:31.936653Z","shell.execute_reply.started":"2022-08-20T21:19:31.929693Z","shell.execute_reply":"2022-08-20T21:19:31.935543Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp_utils.train.TrainEpoch(model=model, \n                                         loss=criterion, \n                                         metrics=metric, \n                                         optimizer=optimizer, \n                                         device=device, \n                                         verbose=True)\n\nvalid_epoch = smp_utils.train.ValidEpoch(model=model, \n                                         loss=criterion, \n                                         metrics=metric, \n                                         device=device)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:31.938788Z","iopub.execute_input":"2022-08-20T21:19:31.939239Z","iopub.status.idle":"2022-08-20T21:19:31.952183Z","shell.execute_reply.started":"2022-08-20T21:19:31.939165Z","shell.execute_reply":"2022-08-20T21:19:31.951113Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"epochs = 1\nmax_score = 0\nPATH_MODEL = './best_model.pth'\nPATH_WEIGHTS = './best_weights.pth'\nscore_break_value = 0.9\n\nfor epoch in range(1,epochs+1):\n    print(f'Current epoch - {epoch} from {epochs}')\n    train_score = train_epoch.run(train_loader)\n    valid_score = valid_epoch.run(val_loader)\n    print(f'Next step.\\n\\n')\n    \n    if max_score < valid_score['iou_score']:\n        max_score = valid_score['iou_score']\n        torch.save(model.state_dict(), PATH_WEIGHTS)\n        torch.save(model, PATH_MODEL)\n        print('Model saved!')\n        \n    if max_score > score_break_value:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:19:31.955012Z","iopub.execute_input":"2022-08-20T21:19:31.955265Z","iopub.status.idle":"2022-08-20T21:29:42.805082Z","shell.execute_reply.started":"2022-08-20T21:19:31.955241Z","shell.execute_reply":"2022-08-20T21:29:42.803741Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"print(f'Test score - {valid_epoch.run(test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:34:47.621703Z","iopub.execute_input":"2022-08-20T21:34:47.622352Z","iopub.status.idle":"2022-08-20T21:37:50.435036Z","shell.execute_reply.started":"2022-08-20T21:34:47.622316Z","shell.execute_reply":"2022-08-20T21:37:50.433544Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for image, mask in test_loader:\n    for i in range(image.shape[0]):\n        predict = model(image[i][None].to(device))[0, ...].cpu().detach()\n        img_show(image=image[i], true_mask=mask[i], predict=predict)\n#         break\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-20T21:38:33.763387Z","iopub.execute_input":"2022-08-20T21:38:33.763774Z","iopub.status.idle":"2022-08-20T21:38:36.766810Z","shell.execute_reply.started":"2022-08-20T21:38:33.763740Z","shell.execute_reply":"2022-08-20T21:38:36.765806Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}